{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use EvoAug to generate OOD mutated sequences given a reference sequence (DeepSTARR).\n",
    "\n",
    "Mutations:\n",
    "- random shuffle: apply `tf.random.shuffle(seqs)` to seqs\n",
    "- mutagenesis: apply `RandomMutation(mutate_frac=0.25)` to seqs\n",
    "- evoaug: apply list of augmentations with `max_augs_per_seq=2` to seqs\n",
    "\n",
    "For each reference sequence from the DeepSTARR test set, we will generate 5 OOD sequences.\n",
    "\n",
    "OOD sequences will be saved as (5,N,L,4) numpy arrays in an h5 file, where N is the number of DeepSTARR test seqs and L is 249 (DeepSTARR input sequence length). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-16 11:15:57.185400: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-16 11:15:57.907241: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.2/lib64:/lib\n",
      "2024-08-16 11:15:57.907321: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.2/lib64:/lib\n",
      "2024-08-16 11:15:57.907328: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# import tensorflow.keras as keras\n",
    "import evoaug_tf\n",
    "from evoaug_tf import evoaug, augment \n",
    "import keras\n",
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "import sys\n",
    "import yaml\n",
    "import h5py\n",
    "sys.path.append('../code')\n",
    "from utils import load_DeepSTARR_data\n",
    "from model_zoo import DeepSTARR\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seeds\n",
    "tf.random.set_seed(1)\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def apply_augment(x, augment_list, hard_aug=True, max_augs_per_seq=2):\n",
    "\n",
    "    if len(x.shape)==2:\n",
    "        x = tf.reshape(x, (1, x.shape[0], x.shape[1]))\n",
    "    \"\"\"Apply augmentations to each sequence in batch, x.\"\"\"\n",
    "    # number of augmentations per sequence\n",
    "    if hard_aug:\n",
    "        batch_num_aug = tf.constant(max_augs_per_seq, dtype=tf.int32)\n",
    "    else:\n",
    "        batch_num_aug = tf.random.uniform(shape=[], minval=1, maxval=max_augs_per_seq+1, dtype=tf.int32)\n",
    "\n",
    "    max_num_aug = len(augment_list)\n",
    "    insert_max = _augment_max_len(augment_list)\n",
    "\n",
    "    # randomly choose which subset of augmentations from augment_list\n",
    "    aug_indices = tf.sort(tf.random.shuffle(tf.range(max_num_aug))[:batch_num_aug])\n",
    "    # apply augmentation combination to sequences\n",
    "    insert_status = True\n",
    "    ind = 0\n",
    "    for augment in augment_list:\n",
    "        augment_condition = tf.reduce_any(tf.equal(tf.constant(ind), aug_indices))\n",
    "        x = tf.cond(augment_condition, lambda: augment(x), lambda: x)\n",
    "        if augment_condition and hasattr(augment, 'insert_max'):\n",
    "            insert_status = False\n",
    "        ind += 1\n",
    "    if insert_status:\n",
    "        if insert_max:\n",
    "            x = _pad_end(x, insert_max)\n",
    "    return x\n",
    "\n",
    "@tf.function\n",
    "def _pad_end(x, insert_max):\n",
    "        \"\"\"Add random DNA padding of length insert_max to the end of each sequence in batch.\"\"\"\n",
    "\n",
    "        N = tf.shape(x)[0]\n",
    "        L = tf.shape(x)[1]\n",
    "        A = tf.cast(tf.shape(x)[2], dtype = tf.float32)\n",
    "        p = tf.ones((A,)) / A\n",
    "        padding = tf.transpose(tf.gather(tf.eye(A), tf.random.categorical(tf.math.log([p] * insert_max), N)), perm=[1,0,2])\n",
    "\n",
    "        half = int(insert_max/2)\n",
    "        x_padded = tf.concat([padding[:,:half,:], x, padding[:,half:,:]], axis=1)\n",
    "        return x_padded\n",
    "    \n",
    "def _augment_max_len(augment_list):\n",
    "    \"\"\"\n",
    "    Determine whether insertions are applied to determine the insert_max,\n",
    "    which will be applied to pad other sequences with random DNA.\n",
    "    Parameters\n",
    "    ----------\n",
    "    augment_list : list\n",
    "        List of augmentations.\n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "        Value for insert max.\n",
    "    \"\"\"\n",
    "    insert_max = 0\n",
    "    for augment in augment_list:\n",
    "        if hasattr(augment, 'insert_max'):\n",
    "            insert_max = augment.insert_max\n",
    "    return insert_max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data from h5 file with hierarchical structure\n"
     ]
    }
   ],
   "source": [
    "data = '../data/DeepSTARR_ensemble_NEW/all_data_with_ensemble_metrics_hierarchical.h5'\n",
    "X_train, y_train, X_test, y_test, X_val, y_val = load_DeepSTARR_data(data, std=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply random shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_random = np.stack([np.random.permutation(X_test) for _ in range(5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 41186, 249, 4)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_random.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply mutagenesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-16 11:16:02.765571: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-16 11:16:03.313884: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14239 MB memory:  -> device: 0, name: NVIDIA RTX A4000, pci bus id: 0000:41:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "X_test_tf = tf.cast(X_test, tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutagenesis = augment.RandomMutation(mutate_frac=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jessica/anaconda3/envs/tf2/lib/python3.9/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
     ]
    }
   ],
   "source": [
    "X_test_mutagenesis = np.stack([mutagenesis(X_test_tf).numpy() for _ in range(5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 41186, 249, 4)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_mutagenesis.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply EvoAug\n",
    "\n",
    "Arguments:\n",
    "- `hard_aug=True`\n",
    "- `max_augs_per_seq=2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "augment_list = [augment.RandomDeletion(delete_min=0, delete_max=20),\n",
    "                augment.RandomTranslocationBatch(shift_min=0, shift_max=20),\n",
    "                # augment.RandomNoise(noise_mean=0, noise_std=0.2),\n",
    "                augment.RandomMutation(mutate_frac=0.05)]\n",
    "\n",
    "# X_test_evoaug = np.stack([tf.map_fn(lambda x: apply_augment(x, augment_list), X_test_tf).numpy()[:,0,:,:] for _ in range(5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "configfile = '../results/DeepSTARR_ensemble_NEW/config.yaml'\n",
    "config  = yaml.safe_load(open(configfile, 'r'))\n",
    "model = evoaug.RobustModel(DeepSTARR, config=config, input_shape=X_test[0].shape, augment_list=augment_list, max_augs_per_seq=2, hard_aug=True)\n",
    "model.compile(keras.optimizers.Adam(learning_rate=0.001, decay=1e-6), loss='mse') \n",
    "\n",
    "# Create a dataset\n",
    "dataset = tf.data.Dataset.from_tensor_slices(X_test_tf)\n",
    "\n",
    "# Batch the dataset\n",
    "batch_size = 1024\n",
    "batched_dataset = dataset.batch(batch_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over batches\n",
    "X_test_evoaug_list = []\n",
    "for i in range(5):\n",
    "    x_mut_list = []\n",
    "    for batch in batched_dataset:\n",
    "        x_mut = model._apply_augment(batch)\n",
    "        x_mut_list.append(x_mut)\n",
    "    X_test_evoaug_list.append(tf.concat(x_mut_list, axis=0))\n",
    "\n",
    "X_test_evoaug = tf.stack(X_test_evoaug_list).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 41186, 249, 4)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_evoaug.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save data to h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf = h5py.File('../data/DeepSTARR/ood_seqs.h5', 'w')\n",
    "\n",
    "hf.create_dataset('random', data=X_test_random)\n",
    "hf.create_dataset('mutagenesis', data=X_test_mutagenesis)\n",
    "hf.create_dataset('evoaug', data=X_test_evoaug)\n",
    "\n",
    "hf.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
